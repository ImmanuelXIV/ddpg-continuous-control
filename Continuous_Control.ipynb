{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning | Continuous Control | Deep Deterministic Policy Gradient (DDPG) agent | Unity Reacher (robot arm) environment\n",
    "\n",
    "---\n",
    "\n",
    "This notebook, shows you how to implement and train an actor-critic [DDPG](https://arxiv.org/abs/1509.02971) (Deep Deterministic Policy Gradient) Reinforcement Learning agent to steer double-jointed robot arms towards target locations in a Unity simulation environment called Reacher.\n",
    "\n",
    "**Why?** Reinforcement Learning (RL) is one of the most fascinating areas of Machine Learning! You might have heared about the breakthrough application [AlphaGo by DeepMind](https://deepmind.com/research/case-studies/alphago-the-story-so-far) which competed in the ancient game of [Go](https://en.wikipedia.org/wiki/Go_(game)) against the (16 times) world champion Lee Sedol and ultimately won 4-1.  RL is quite intuitive, because we use positive and negative feedback to learn tasks via interaction - just like we would train a dog. From controlling robots to stock trading to solving complex simulated tasks, RL has many applications and bears lots of potential. This notebook is a project submission for the [Udacity Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). My contribution lays mainly in section 1, 5, 6, and 7.\n",
    "\n",
    "**What?** The [Unity](https://en.wikipedia.org/wiki/Unity_(game_engine)) environment shown in this notebook is called [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md). In this episodic task the agent can control the movement of 20 double-jointed robot arms and the goal is to move each robot hand into a green target location and keep it there. If this is achieved the agent receives positive reward of +0.1 each time step and ultimately a positive game core. This is what a trained agent looks like!\n",
    "\n",
    "<img src=\"imgs/agent.gif\" width=\"450\" align=\"center\" title=\"reacher unity environment\"/>\n",
    "\n",
    "**How?** This notebook will step you through the process how to build and train a DDPG agent! Keep reading.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Learning Algorithm\n",
    "\n",
    "The Deep Deterministic Policy Gradient aka [DDPG algorithm](https://arxiv.org/abs/1509.02971) by Lillicrap et al. (2015) is an actor-critic, off-policy, model-free Reinforcement Learning approach. It extends the renown [DQN algorithm](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) to solve environments with continuous action spaces and further builds on top of its older brother the [DPG algorithm](http://proceedings.mlr.press/v32/silver14.pdf) algorithm by Silver et al. (2014). Let's briefly dissect the concepts mentioned: \n",
    "\n",
    "**Actor-critic** methods have an actor model to select actions, and a critic model to criticize the action selection to make it better. They combine critic-only and actor-only methods. Critic-only methods are Value-Based methods. They aim to find the optimal (action) value function first and then derive an optimal policy from it by, for instance, selecting the (epsilon-) greedy action. This is the action that is likely to yield the most expected return. Actor-only methods are Policy-Based methods, which search for the optimal policy directly, without calculating the (action) value function first. In actor-critic methods the actor model learns a policy to determine the next action probabilities and the critic model learns a value function that is used to determine how well the actor policy is performing. An advantage of actor-critic models is that they usually have good convergence properties.\n",
    "\n",
    "In **off-policy** methods the policy used for interacting with an environment is not the policy that is being learned. The off-policy agent aims to learn the optimal policy (which yields the maximum expected reward) directly, though the policy it has at hand to interact with the environment is not optimal yet and therefore different. In fact the policy might be a random policy in the beginning. An example algorithm is DQN (checkout my DQN notebook [here](https://github.com/ImmanuelXIV/dqn-navigate/blob/master/Navigation.ipynb) if you are interested). On-policy methods, on the other hand, aim to optimize the same policy they use for interacting with the environment.\n",
    "\n",
    "**Model-free** Reinforcement Learning tasks are framed as (finite) Markov Decision Processes (MDPs). MDPs consist of: a (finite) set of states $S$, a (finite) set of actions $A$, a set of rewards $R$, the discount rate $\\gamma$, and the one-step dynamics of the environment. In model-free approaches the agent does not have access to the function that yields the reward $R(s,a)$, and neither to the one-step transition dynamic $T(s'|s,a)$ that returns the next state $s'$. If the agent has access to them as well it is a model-based approach, which can utilize planning algorithms for decision-making.\n",
    "\n",
    "**Continuous action space** means that the actions our agent controlls are not e.g. four discrete \"left, right, up, down\" actions but they can take continuous values in intervals of let's say [-1 1], [-7.9 7.9], [-37.8 37.8], [-23 23]. Since there are unlimited different combinations of continuous actions this makes the learning task much difficult. Constinuous action spaces are common in robot motion tasks. \n",
    "\n",
    "Underneath is the DDPG algorithm taken from [Lillicrap et al.](https://arxiv.org/abs/1509.02971). Please refer to the paper for more details.\n",
    "\n",
    "<img src=\"imgs/ddpg-algorithm.png\" width=\"600\" align=\"center\" title=\"ddpg algorithm\"/>\n",
    "\n",
    "The chosen hyperparameters for the actor and critic models, the noise function, and the replay buffer are as follows. Refer to the code for more details.\n",
    "\n",
    "- SEED = 123\n",
    "- GAMMA = 0.98\n",
    "- LR_A = 1e-4\n",
    "- LR_C = 1e-3\n",
    "- WEIGEHT_DECAY = 0\n",
    "- TAU = 1e-3\n",
    "- BATCH_SIZE = 128\n",
    "- C = 20\n",
    "- D = 10\n",
    "---\n",
    "- mu=0.0\n",
    "- theta=0.15\n",
    "- sigma=0.2\n",
    "---\n",
    "- BUFFER_SIZE = 1e6\n",
    "\n",
    "\n",
    "The chosen model architecture for the actor and critic neural networks are:\n",
    "\n",
    "```python\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=33, out_features=200, bias=True)\n",
    "  (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (fc2): Linear(in_features=200, out_features=300, bias=True)\n",
    "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (fc3): Linear(in_features=300, out_features=4, bias=True)\n",
    ")\n",
    "\n",
    "Critic(\n",
    "  (fc1): Linear(in_features=33, out_features=200, bias=True)\n",
    "  (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (fc2): Linear(in_features=204, out_features=300, bias=True)\n",
    "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start the Environment\n",
    "\n",
    "Begin by importing the necessary packages. If the code cell below returns an error, please revisit the project instructions in the `README.md` file to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded (see `README.md`). For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is uncommented and executed, you will watch the agent's performance, and how it selects an action at random each time step.  A window should pop up that allows you to observe the agent as it moves the robot arms in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \\nstates = env_info.vector_observations                  # get the current state (for each agent)\\nscores = np.zeros(num_agents)                          # initialize the score (for each agent)\\nwhile True:\\n    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\\n    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\\n    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\\n    next_states = env_info.vector_observations         # get next state (for each agent)\\n    rewards = env_info.rewards                         # get reward (for each agent)\\n    dones = env_info.local_done                        # see if episode finished\\n    scores += env_info.rewards                         # update the score (for each agent)\\n    states = next_states                               # roll over states to next time step\\n    if np.any(dones):                                  # exit loop if episode finished\\n        print('Episode finished.') \\n        break\\nprint('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\\nenv.close()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        print('Episode finished.') \n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "env.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train a DDPG agent \n",
    "\n",
    "I'd recommend that you open the files `ddpg_agent.py`, `ac_models.py`, `noise.py`, and `replaybuffer.py` in different tabs and walk through the code step by step to gather a deeper understanding. Running this code until the environment was solved took approximately 00:35h on a Nvidia Tesla K80 gpu and about 01:20h on a 2.7 GHz Intel Core i5. If you just want to see a trained agent controlling the robot arms, then jump to section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from ddpg_agent import Agent\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "num_episodes = 200      # number of episodes to train\n",
    "t_max = 1000            # maximum number of time steps per episode\n",
    "print_every = 10        # print states every couple of steps\n",
    "\n",
    "def run_ddpg(agent, env):\n",
    "    \"\"\" Initialize the agent.\n",
    "        Params\n",
    "        ======\n",
    "            agent:      Agent object\n",
    "            env:        Unity Reacher environment\n",
    "    \"\"\"\n",
    "    scores_all = list()\n",
    "    scores_window = deque(maxlen=100)\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "        states = env_info.vector_observations                  # get current state (each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize score (each agent)\n",
    "        agent.noise.reset()                                    # reset noise each episode\n",
    "        \n",
    "        for t in range(t_max):\n",
    "            actions = agent.select_action(states)              # select actions\n",
    "            actions = np.clip(actions, -1, 1)                  # clip actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to env\n",
    "            next_states, rewards, dones = env_info.vector_observations, env_info.rewards, env_info.local_done\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            scores += rewards                                  # update the score (each agent)\n",
    "            states = next_states                               # roll over to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "        scores_all.append(np.mean(scores))\n",
    "        scores_window.append(np.mean(scores))\n",
    "        \n",
    "        print('\\r{}/{} Episode. Total score (averaged over agents): {:.3f}'.format(i_episode, num_episodes, \\\n",
    "                                                                               np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\r{}/{} Episode. Total score (averaged over agents): {:.3f}'.format(i_episode, num_episodes, \\\n",
    "                                                                               np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 30:\n",
    "            print('\\rEnvironment solved after {} episodes. Total score (averaged over agents): {:.3f}'\\\n",
    "                  .format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoints/checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoints/checkpoint_critic.pth')\n",
    "            break\n",
    "        \n",
    "    return scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/200 Episode. Total score (averaged over agents): 1.218\n",
      "20/200 Episode. Total score (averaged over agents): 2.211\n",
      "30/200 Episode. Total score (averaged over agents): 5.434\n",
      "40/200 Episode. Total score (averaged over agents): 10.224\n",
      "50/200 Episode. Total score (averaged over agents): 15.009\n",
      "60/200 Episode. Total score (averaged over agents): 18.178\n",
      "70/200 Episode. Total score (averaged over agents): 20.369\n",
      "80/200 Episode. Total score (averaged over agents): 21.968\n",
      "90/200 Episode. Total score (averaged over agents): 23.436\n",
      "100/200 Episode. Total score (averaged over agents): 24.259\n",
      "110/200 Episode. Total score (averaged over agents): 26.720\n",
      "120/200 Episode. Total score (averaged over agents): 29.014\n",
      "Environment solved after 126 episodes. Total score (averaged over agents): 30.017\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size, action_size)\n",
    "scores = run_ddpg(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time a score >= 30 was reached at episode 39.\n",
      "Max score reached:  37.52649916121736\n"
     ]
    }
   ],
   "source": [
    "scores = np.array(scores)\n",
    "x = np.where(scores >= 30)\n",
    "print('The first time a score >= 30 was reached at episode {}.'.format(x[0][0]))\n",
    "print('Max score reached: ', np.amax(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Scores')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Watch a trained agent\n",
    "\n",
    "Run the code cell below to load a trained agent and run it in the environment. You might have to restart the notebook Kernel and only run the code in sections 2 and 3, before running the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished.\n",
      "Episode 1 finished.\n",
      "Total score (averaged over agents) this episode: 33.791499244701114\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "num_episodes = 2\n",
    "\n",
    "agent = Agent(state_size, action_size)\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoints/checkpoint_actor.pth', \\\n",
    "                                             map_location=torch.device('cpu')))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoints/checkpoint_critic.pth', \\\n",
    "                                              map_location=torch.device('cpu')))\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]         # reset the environment    \n",
    "    states = env_info.vector_observations                      # get current state (each agent)\n",
    "    scores = np.zeros(num_agents)                              # initialize score (each agent)\n",
    "    \n",
    "    while True:\n",
    "        actions = agent.select_action(states, ou_noise=False)  # select actions\n",
    "        actions = np.clip(actions, -1, 1)                      # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]               # send all actions to env\n",
    "        next_states = env_info.vector_observations             # get next state (each agent)\n",
    "        rewards = env_info.rewards                             # get reward (for each agent)\n",
    "        dones = env_info.local_done                            # see if episode finished\n",
    "        scores += env_info.rewards                             # update score (each agent)\n",
    "        states = next_states                                   # roll over to next time step\n",
    "        if np.any(dones):                                      # exit loop if epis. finished\n",
    "            print('Episode {} finished.'.format(i_episode)) \n",
    "            break\n",
    "            \n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ideas for future work\n",
    "\n",
    "To reach even better results one could tweak the network architectures of the actor and critic in `ac_models.py`, and as well, focus on e.g. batch normalization, the noise function, and the update parameters C, and D, which had a strong impact on learning performance of the DDPG agent. More simulation time might yield slightly better results. To speed up learning one could train similar ddpg agents on multiple gpus and then average the learned network parameters for one final ddpg agent. Another approach would be to implement a different algorithm such as [PPO](https://arxiv.org/pdf/1707.06347.pdf), [A3C](https://arxiv.org/pdf/1602.01783.pdf), and [D4PG](https://openreview.net/pdf?id=SyZipzbCb) which use multiple (non-interacting, and parallel) copies of the same agent to distribute the task of gathering experience.  \n",
    "\n",
    "This notebook is a project submission for the [Udacity Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893). My contribution lays mainly in section 1, 5, 6, and 7. If you are interested to learn more about Deep Reinforcement Learning, I can highly recommend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
